{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Carrying out error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "- To carry out error analysis, you should find **a set of mislabeled examples** in your dev set. And look at the mislabeled examples for **false positives** and **false negatives**. And just count up the number of errors that fall into various different categories. During this process, you might be inspired to generate new categories of errors. If you're looking through the examples and you say, there are a lot of Instagram filters, or Snapchat filters, they're also messing up my classifier. You can create new categories during that process. But by counting up the fraction of examples that are mislabeled in different ways, often this will help you prioritize. Or give you inspiration for new directions to go in.\n",
    "\n",
    "![](./imgs/error-analysis1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cleaning up incorrectly labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incorrectly Labeled Samples:**\n",
    "\n",
    "- DL algorithms are quite robust to random errors in the training set but less robust in systematic errors. \n",
    "\n",
    "- **Error analysis**: if the proportion of errors due incorrect labels is large, need to modify train and test set.\n",
    "\n",
    "\n",
    "**Correcting incorrect dev/test set examples**:\n",
    "\n",
    "- apply same process to dev&test sets so that they are crom teh same distribution\n",
    "\n",
    "- consider examining examples your algorithm got right as well as ones it got wrong\n",
    "\n",
    "- Train and dev/test data may now come from slightly different distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Build your first system quickly, then iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**:\n",
    "- Set up dev/test set and metric\n",
    "- Build initial system quickly\n",
    "- Use Bias/Variance analysis & Error analysis to prioritize next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mismatched Training and Dev/Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Training and testing on different distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario: mobile app image classifiaction**\n",
    "- classify low-resolution mobile app derived images, also a small amount\n",
    "- have high-resoluton images crawled online, a great amount\n",
    "\n",
    "![](./imgs/mistached-train-test-distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option1**:\n",
    "- Advantage: both dev and test come from the same distributions \n",
    "- Disadvantage: the larger proportion in dev/test are high resolution images which are not the target\n",
    "- **reject**!\n",
    "\n",
    "**Option2**:\n",
    "- Advantage: hit the target: mobile app images\n",
    "- Disadvantage: the distribution of train set is different from dev/test set\n",
    "- **Accepted**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another scenario**:\n",
    "\n",
    "![](./imgs/mistached-train-test-distribution1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Bias and Variance with mismatched data and distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error analysis in mismatched data:**\n",
    "- Def: **Train-dev set**, which is drawn from the training set but not train on it, only with the same distributions. \n",
    "- Set human error: ~0%\n",
    "\n",
    "\n",
    "1. train error = 1%, train-dev error = 9%, dev error = 10% \n",
    "    - high variance, overfitting the train set\n",
    "2. train error = 1%, train-dev error = 1.5%, dev error = 10%\n",
    "    - data mismatch issue\n",
    "3. train error = 10%, train-dev error = 11%, dev error = 12%\n",
    "    - avoidable bias, underfitting the train set\n",
    "4. train error = 10%, train-dev error = 11%, dev error = 20%\n",
    "    - avoidable bias + data mismatch\n",
    "    \n",
    "![](./imgs/mistached-train-test-error-analysis.PNG)\n",
    "\n",
    "\n",
    "![](./imgs/mistached-train-test-error-analysis-exp.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Addressing data mismatch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempts:**\n",
    "- carry out manual error analysis to try to understand difference between training adn dev/test sets\n",
    "- make training data more similar: or collect more data similar to dev/test sets\n",
    "    - **artificial data synthesis**\n",
    "        - potential drawback: only synthesizing a small part of the overall label set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Learning from Multiple Tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def:**\n",
    "- Pre-training: use the pre-trained neural networks for other tasks\n",
    "- Fine-tuining: tune the weights for the target object\n",
    "- **The reason is that the frist layers usually extract the nature of images such as the poitns/edges/cornors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When transfer learning makes sense:**\n",
    "- When transfer learning works is that there are a lot of data you're transfering from but usually relatively less data for the problem you're transferring to.\n",
    "- **Low level features** from A could be helpful for learning B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def:**\n",
    "- Unlike Softmax which assigns single label to an object, one object can have many labels in multi-task learning.\n",
    "- e.g., self-driving car detection: four vectors for an image (pedestrain, car, stop sign, traffic lights)\n",
    "- We can train several models to classify each vector but if the first layers have the same low-level features, we can use them together in multi-task learning. \n",
    "    - In the label set, if some objects do not have the labels for certain attribute, we can just omit them in classification and cost function.\n",
    "\n",
    "![](./imgs/multi-task-learning.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When multi-task learning makes sense:**\n",
    "- Training on a set of tasks that could benefit from having shared lower-level features\n",
    "- Ususlly: amount of data you have for each task is quite similar\n",
    "    - it can boost the train data for each samples\n",
    "- Can train a big enough neural network to do well on all the tasks\n",
    "\n",
    "    ![](./imgs/multi-task-learning-ad.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. End-to-end Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 What is end-to-end deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Def:**\n",
    " - An end-to-end model learns all the features that can occur between the original inputs (x) and the final outputs (y).\n",
    " - A machine learning model can directly convert an input data into an output prediction bypassing the intermediate steps that usually occur in a traditional pipeline.\n",
    " \n",
    "    ![](./imgs/end2end-exp1.PNG)\n",
    "    \n",
    "    ![](./imgs/end2end-exp2.PNG)\n",
    "    \n",
    "    ![](./imgs/end2end-exp3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Whether to use end-to-end deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "- Let the data speak\n",
    "- Less hadn-designing of components needed\n",
    "\n",
    "**Cons:**\n",
    "- May need large amount of data\n",
    "- Excludes potentially useful hand-designed components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying end-to-end deep learning:**\n",
    "- **key question**: do you have sufficient data to learn a function of the complexity needed to map x to y? \n",
    "- Limited in some complex cases such as self-driving car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. \"Based on table from the previous question, a friend thinks that the training data distribution is much easier than the dev/test distribution. What do you think?\"**\n",
    "\n",
    "- The algorithm does better on the distribution of data it trained on. But you don’t know if it’s because it trained on that no distribution or if it really is easier. To get a better sense, measure human-level error separately on both distributions.\n",
    "- In other words, if human-level error is smaller, the distribution is easy to learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No assignment in this week. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
