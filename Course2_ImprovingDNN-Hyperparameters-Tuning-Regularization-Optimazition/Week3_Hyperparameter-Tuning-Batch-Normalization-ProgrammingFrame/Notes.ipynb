{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tuning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance to Tune:** \n",
    "1. Learning rate\n",
    "2. $\\beta$, #hidden units, mini-batch size\n",
    "3.  #layers, leraning rate decay\n",
    "4. Adam parameters (using defaults) \n",
    "\n",
    "Searching Tips: \n",
    "- Dont' use a grid searchign rather than randomly select\n",
    "- Coarse to fine \n",
    "    - Search in the coarse area first, then focus on the fine area to have a dense searching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Using an Appropriate Scale to Pick Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How random is it?**\n",
    "- Uniformly rando, i.e., linear scale (X)\n",
    "- Log scale is better\n",
    "\n",
    "```python\n",
    "# sample learning rate\n",
    "r = -4 * np.random.rand()\n",
    "learning_rate = 10**r\n",
    "\n",
    "# sample beta in EWA\n",
    "r = -3 * np.random.rand()\n",
    "beta = 10**r\n",
    "\n",
    "```\n",
    "\n",
    "* Goal is to smaple the hyperparameters more efficiently. Uniform random is not too bad, but using a log scale can make the searching faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Hyperparameters Tuning in Practice: Pandas vs. Caviar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Re-test hyperparameters occasionally \n",
    "- Babysitting one model \n",
    "    - everyday trains a small model and keep tuning\n",
    "- Training many models in parallel\n",
    "    - find one that performs best \n",
    "\n",
    "![](./imgs/pandas-caviar.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Normalizing Activations in a Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Norm:**\n",
    "- Normalize the activation inputs ($Z^{[l]}$) so as to train the parameters in the next layer faster. \n",
    "\n",
    "- for a certain layer $l$ : \n",
    "\n",
    "$$ u = \\frac{1}{m} \\; \\sum_{i} \\; z^{(i)} $$\n",
    "$$ \\sigma^{2} = \\frac{1}{m} \\; \\sum_{i} \\; (z_{i} - u)^{2} $$\n",
    "$$ z_{norm}^{(i)} = \\frac{z^{(i)} - u}{\\sqrt{\\sigma^2 + \\epsilon}} $$\n",
    "\n",
    "$$ \\tilde{z}^{(i)} = \\gamma \\; z_{norm}^{(i)} + \\beta $$\n",
    "\n",
    "where $\\gamma, \\beta$ are learnable parameters of models \n",
    "\n",
    "**Note:**\n",
    "\n",
    "- In some deeper layers, we want the values are still can learn faster \n",
    "- But, at the same time, we do not want the mean and the variance of the deeper outputs are forced to be 0 and 1. Thus, we add two learnable parameters so that each hidden unit values can learn different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fitting Batch Norm into a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\beta^{[l]}, \\gamma^{[l]}$ parameters are added to the model **\n",
    "- **They are not hyperparameters needed to tune but the parameters needed to update**.\n",
    "    - Updates include $dW^{[l]}, d\\gamma^{[l]}, d\\beta^{[l]}$\n",
    "- Use some optimization algorithms to update these two learnable parameters.\n",
    "- In differen mini-batches, $\\beta, \\gamma$ only rely on the current mini-batch.\n",
    "\n",
    "**Note:**\n",
    "- In Bath Norm, the parameter $b$ can be removed and end up with only using the parameter $beta$, because 5b5 is just a number added to $z$ and will be removed in the normalization part. Thus, we can only use one parameter $ \\beta$ to control the mean. \n",
    "\n",
    "$$Z^{[l]} = W^{[l]} a^{[l-1]} $$\n",
    "\n",
    "$$ Z^{[l]}_{norm} $$\n",
    "\n",
    "$$ \\tilde{Z}^{[l]} = \\gamma^{[l]} Z^{[l]}_{norm} + \\beta^{[l]} $$\n",
    "\n",
    "where $\\gamma^{[l]}, \\beta^{[l]}$ are all $(n^{[l]}, 1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Why does Batch Norm Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Like normalizing the train set, Batch Norm can make the hidden units learn faster.** \n",
    "2. **Make the deeper layers more roboust than the previous layers**. Limit the amount of the impacts from the prevous layers on the distribution of mean and variance in the current layer. \n",
    "    - The mean and variance are influented by the previous layers. \n",
    "    - Batch Norm can make the change still around 0 mean and 1 variance so that the distirbution are \"stable\".\n",
    "    - Thus, Batch Norm can reduce the problem of the input values changing, so **the later layers are not too adaptive as the previous layers**. Hence, the coupling problem can be solved and each layer can learn independently. \n",
    "3. **Have a slight regularization effect.** Each mini-batch is scaled by the mean/variance computed on just that mini-batch. Thus, this adds some noise to the values within that mini-batch, just like what dropout does.        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Batch Norm at Test Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since at test time we cannot compute the mean/variance for each test sample, so what we do is to **estimate $u, \\sigma^{2}$ using exponentially weighted average across mini-batches**.\n",
    "\n",
    "- From the train set, we can obtian: \n",
    "$$ u^{\\{1\\}[l]}, u^{\\{2\\}[l]}, u^{\\{3\\}[l]}, ....  $$\n",
    "$$ \\sigma^{2 \\; \\{1\\}[l]}, \\sigma^{2 \\; \\{2\\}[l]}, \\sigma^{2 \\; \\{3\\}[l]}, ....  $$\n",
    "- Thus, using exponentially weighted average to obtain the $u, \\sigma^{2}$ for that layer at test time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation:**\n",
    "- C = #classes = $n^{[L]}$ \n",
    "- Output Size: $\\hat{Y} : (C, m)$\n",
    "\n",
    "**Softmax Regression:**\n",
    "$$ Z^{[L]} = W^{[L]} \\; a^{[L]} + b^{[L]} $$\n",
    "\n",
    "$$ t = e^{Z^{[L]}} $$\n",
    "$$ a^{[L]} = \\frac{t}{\\sum_{j} \\; t_{j}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training a Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Softmax:**\n",
    "- Softmax regression generalizes logistic regression to C classes. \n",
    "\n",
    "\n",
    "**Lost Function:**\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}, y) = - \\sum_{j=1}^{C} \\; y_{j}\\log{\\hat{y}_{j}} $$\n",
    "\n",
    "\n",
    "**Cost Function:**\n",
    "\n",
    "$$ J(W^{[1]}, b^{[1]}, ...) = \\frac{1}{m} \\; \\sum_{i=1}^{m} \\; \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) $$\n",
    "\n",
    "\n",
    "**Derivatives of Softmax Regression:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dZ^{[L]} = \\frac{\\partial{J}}{\\partial{Z^{[L]}}} =  \\hat{y} - y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Introduction to Programming Frameworks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning Framework:**\n",
    "- Caffe/Caffe2\n",
    "- CNTK\n",
    "- DL4L \n",
    "- Keras\n",
    "- Lasagne\n",
    "- Mxnet\n",
    "- PaddlePaddle\n",
    "- TensorFlow\n",
    "- Theano\n",
    "- Torch\n",
    "\n",
    "Careful to: \n",
    "- ease of programming\n",
    "- running speed\n",
    "- truly open with good governance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow**\n",
    "\n",
    "- An efficient tool which only need to denote the forward propogation while the backward propogation will be automatically computed using the built-in functions.\n",
    "\n",
    "\n",
    "- i.e. how to minimize the cost function \n",
    "\n",
    "![](./imgs/tensorflow-eg.jpg)\n",
    "\n",
    "**Note:**\n",
    "- `with` command is better at cleaning up in cases an error in exception while executing this inner loop. \n",
    "- `tf.placeholder` is a holder in a formular while `feed_dict={x:coef}` is to feed the data to the place holder. By doing this can make the programming easier to switch input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Norm:**\n",
    "\n",
    "- There is no optimal combination of $\\gamma, \\beta$. Both need to be turned and rely on the minimal cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Operations in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Run an Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(10)\n",
    "c = tf.multiply(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "Tensor(\"Mul:0\", shape=(), dtype=int32)\n",
    "```\n",
    "\n",
    "\n",
    "As expected, you will not see 20! You got a tensor saying that the result is a tensor that does not have the shape attribute, and is of type \"int32\". \n",
    "- **All you did was put in the 'computation graph', but you have not run this computation yet.** \n",
    "- In order to actually multiply the two numbers, you will have to create a session and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "20\n",
    "```\n",
    "\n",
    "To summarize, remember to **initialize your variables, create a session and run the operations inside the session**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the value of x in the feed_dict\n",
    "\n",
    "x = tf.placeholder(tf.int64, name = 'x')\n",
    "print(sess.run(2 * x, feed_dict = {x: 3}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "6\n",
    "```\n",
    "\n",
    "When you first defined x you did not have to specify a value for it. **A placeholder is simply a variable that you will assign data to only later, when running the session.** We say that you feed data to these placeholders when running the session.\n",
    "\n",
    "Here's what's happening: When you specify the operations needed for a computation, you are telling TensorFlow how to construct a computation graph. The computation graph can have some placeholders whose values you will specify only later. Finally, when you run the session, you are telling TensorFlow to execute the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sign Recognization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**\n",
    "\n",
    "- To teach our computers to decipher sign language. It's now your job to build an algorithm that would facilitate communications from a speech-impaired person to someone who doesn't understand sign language.\n",
    "\n",
    "\n",
    "**Data:**\n",
    "- Training set: 1080 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (180 pictures per number).\n",
    "- Test set: 120 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (20 pictures per number).\n",
    "\n",
    "Note that this is a subset of the SIGNS dataset. The complete dataset contains many more signs.\n",
    "\n",
    "Here are examples for each number, and how an explanation of how we represent the labels. These are the original pictures, before we lowered the image resolutoion to 64 by 64 pixels.\n",
    "\n",
    "![](./imgs/hw-data.png)\n",
    "\n",
    "\n",
    "**Model:**\n",
    "- Two hidden layers with ReLU and one output layer with softmax.\n",
    "- Hidden units are [25, 12, 6]\n",
    "- Learning rate: 0.0001\n",
    "- #epochs: 1500\n",
    "- Mini-batch size: 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Note:**\n",
    "1. Make sure the data type of the parameters is the same as the input data. When doing the initialization and propogation, one can set them consistent: \n",
    "\n",
    "```python\n",
    "# During parameter initialization\n",
    "W1 = tf.get_variable(\"W1\", [25, 12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1, dtype = tf.float32))\n",
    "b1 = tf.get_variable(\"b1\", [25, 1], initializer = tf.zeros_initializer(dtype = tf.float32))\n",
    "\n",
    "# During forward propogation\n",
    "Z1 = tf.add(tf.matmul(W1, tf.cast(X, tf.float32)), b1)\n",
    "```\n",
    "\n",
    "2. Some functions require the name of the input parameters, so don't ignore them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy:**\n",
    "\n",
    "![](./imgs/hw-acc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- Tensorflow is a programming framework used in deep learning\n",
    "- The two main object classes in tensorflow are Tensors and Operators. \n",
    "- When you code in tensorflow you have to take the following steps:\n",
    "    - Create a graph containing Tensors (Variables, Placeholders ...) and Operations (tf.matmul, tf.add, ...)\n",
    "    - Create a session\n",
    "    - Initialize the session\n",
    "    - Run the session to execute the graph\n",
    "- You can execute the graph multiple times as you've seen in model()\n",
    "- The backpropagation and optimization is automatically done when running the session on the \"optimizer\" object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
