{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up your Machine Learning Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Train/dev/test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:**\n",
    "\n",
    "- Training set\n",
    "\n",
    "- Hold-out/cross validation/development set\n",
    "    \n",
    "    - to see which of the model perform the best\n",
    "    \n",
    "- Test set\n",
    "\n",
    "**Strategy in Splitting Data**\n",
    "\n",
    "    - several years ago: 70/30 or 60/20/20\n",
    "    \n",
    "    - big data era: 99/1/1 (i.e. 10k for validation and test are enough for an entire data set over 1m samples)\n",
    "    \n",
    "**Guideline**\n",
    "\n",
    "    - Make sure the dev and test sets come from the same distribution.\n",
    "    \n",
    "    - (In some cases, not having a test set might be okay, only having dev set.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Bias/Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Train & Dev Set Errors to Determine:**\n",
    "\n",
    "- Assume that the human classification error / optimal error / Bayes error $:\\approx $ 0%\n",
    "\n",
    "- **High Variance (underfitting)**: 1% train set error but 11% dev set error\n",
    "\n",
    "- **High Bias (overfitting)**: 15% train set error but 16% dev set error\n",
    "\n",
    "- **High Bias & Variance**: 15% train set error and 30% dev set error\n",
    "\n",
    "- **Low Bias & Variance**: 0.5% train set error and 1% dev set error\n",
    "\n",
    "\n",
    "- Example: \n",
    "\n",
    "![](./imgs/bias-variance.jpg)\n",
    "\n",
    "![](./imgs/high-bv.jpg)\n",
    "\n",
    "- High bias is because the most parts are linear\n",
    "- High variance is because of too much flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Basic Recipe for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the most efficient way to reduce high bias and viriance.** \n",
    "\n",
    "- First, check if the bias is high?\n",
    "    - bigger network (more layers and hidden unites)\n",
    "    - train longer \n",
    "    - search NN architecture\n",
    "\n",
    "\n",
    "- Check if the variance is high? \n",
    "    - more data \n",
    "    - regularization \n",
    "    - NN architecture search         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Bias Variance Trade-off\"**\n",
    "\n",
    "- In the earlier era of machine learning, one cannot reduce either bias or variance without really hurting the other.\n",
    "\n",
    "- In the modern era, using the recipe above can drive one of them down without necessarily hurting the other too much.\n",
    "    - regularization \n",
    "    - bigger network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regularization you Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$L_{2}$ Regularization:**\n",
    "\n",
    "In logistic regression: we want to further minimize $J(w,b)$, where\n",
    "\n",
    "$$ J(w,b) = \\frac{1}{m} \\; \\sum_{i=1}^{m} \\; \\mathcal{L}(\\hat{y}^{(i)}, \\; y^{(i)}) + \\frac{\\lambda}{2m} \\; ||w||^{2}_{2} $$\n",
    "\n",
    "$$ \\frac{\\lambda}{2m} \\; ||w||^{2}_{2} = \\frac{\\lambda}{2m} \\; \\sum_{j=1}^{n_{x}} w_{j}^{2} = \\frac{\\lambda}{2m} \\; w^{T}w$$\n",
    "\n",
    "- $\\lambda$ is the **regularization parameter**. It is set based on the dev set in order to prevent the overfitting.\n",
    "    - In python programming, lambda is a reversed key word, so using other name, i.e. lambd, to represent the regularization parameter.\n",
    "\n",
    "Why omit the bias term $b$: \n",
    "\n",
    "- $w \\in \\mathbb{R}^{n_{x}}$ is a high dimensional parameter while $b \\in \\mathbb{R}$ is only one dimensional variable, so in practice, adding the regularization of $b$ cannot improve the performance too much.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$L_{1}$ Regularization:**\n",
    "\n",
    "$$ \\frac{\\lambda}{2m} \\; \\sum_{j=1}^{n_{x}} |w_{j}| = \\frac{\\lambda}{2m} \\; ||w||_{1} $$\n",
    "\n",
    "- It ends up with being sparse, which means it will compress the model and reduce some memories. In practice, it does not help a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization in Neural Network:**\n",
    "\n",
    "$$ J(W^{[1]}, b^{[1]}, ..., W^{[L]}, b^{[L]}) = \\frac{1}{m} \\; \\sum_{i=1}{m} \\; \\mathcal{L} (\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\; \\sum_{l=1}^{L} ||W^{[l]}||^{2}_{F} $$\n",
    "\n",
    "where $$ ||W^{[l]}||^{2}_{F} = \\sum_{i=1}^{n^{[l-1]}} \\; \\sum_{j=1}^{n^{[l]}} \\; (W^{[l]}_{ij})^{2} \\text{, called Forbenius / Euclidean Norm} $$\n",
    "\n",
    "where $ W^{[l]} : (n^{[l]}, \\; n^{[l-1]}) $\n",
    "\n",
    "\n",
    "**Updates in Gradient Descent:**\n",
    "\n",
    "$$ dW^{[l]} = (\\text{from back propogation}) + \\frac{\\lambda}{m} \\; W^{[l]} $$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\; dW^{[l]} $$\n",
    "\n",
    "$$ W^{[l]} = (1- \\frac{\\alpha \\lambda}{m}) W^{[l]} - \\alpha \\; (\\text{from back propogation}) $$\n",
    "\n",
    "Since $W^{[l]}$ is keep smaller in each iteration, $L_{2}$ regularization is also called **\"Weight Decay\"**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Why does Regularization Reduce Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assume $\\lambda$ is very large**: \n",
    "\n",
    "- In each update, $W^{[l]} \\approx 0$. To some extent, many hidden units are killed in the big neural network, and eventually, the model will be simplified as a simple logistic regression, which has very low variance but high bias. \n",
    "\n",
    "- In this way, we can tune the regularization parameter which can control the impacts of parameters on the model so that the model is right in the place where bias and variance are both small.\n",
    "\n",
    "![](./imgs/large-lambda.jpg)\n",
    "    \n",
    "**Another interpretation:**\n",
    "\n",
    "- When $\\lambda$ is very large, in the activation function $tanh()$, the smaller parameter $W^{[l]}$ will lead to a smaller range of $Z^{[l]}$. Thus, the slope of $tanh$ is vert closed to a linear function. Hence, the whole neural network is not far away from a linear combination, which can prevent overfitting / too much flexibility of the model.\n",
    "\n",
    "    ![](./imgs/large-lambda-tanh.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dropout Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:**\n",
    "\n",
    "In each iteration, for each layer, for each training sample, we set a probability to eliminish hidden units and use the eliminished network to train that specific training sample in current layer and iteration.\n",
    "\n",
    "**Implementing Dropout (\"Inverted Dropout\"):**\n",
    "\n",
    "In a iteration, one layer i: \n",
    "\n",
    "```python\n",
    "keep_prob = 0.8 # the percent of units that will be removed\n",
    "\n",
    "di = np.random.rand(Ai.shape[0], Ai.shape[1]) < keep_prob # generate a binary filter\n",
    "\n",
    "Ai = np.multiply(Ai, di) # remove units \n",
    "\n",
    "Ai = Ai / keep_prob \n",
    "```\n",
    "\n",
    "The last line (\"inverted dropout\")\n",
    "\n",
    "- is used to pump up the expected value of the input data so that in the computation of forward propogation, the expected value of Z won't change\n",
    "- also makes **the test process easier** because it reduces some scaling problem.\n",
    "\n",
    "\n",
    "**Backward propogation:** \n",
    "- You had previously shut down some neurons during forward propagation, by applying a mask $D^{[i]}$ to `Ai`. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask $D^{[i]}$ to `dAi`. \n",
    "\n",
    "- During forward propagation, you had divided `Ai` by `keep_prob`. In backpropagation, you'll therefore have to divide `dAi` by `keep_prob` again (the calculus interpretation is that if $A^{[i]}$ is scaled by `keep_prob`, then its derivative $dA^{[i]}$ is also scaled by the same `keep_prob`).\n",
    "\n",
    "\n",
    "**Strategy:**\n",
    "\n",
    "- Different layers can have different keep-prob. Usually, the larger the weight matrix, the lower the keep-prob. By doing this, we can solve the issue where some layers are more likely to be overfitting than others. \n",
    "\n",
    "- For some layers which only have 1 or 2 hidden uints, the keep-prob can be 1. \n",
    "\n",
    "    ![](./imgs/diff-keepprob.jpg)\n",
    "\n",
    "**Mking prediction at test time:**\n",
    "\n",
    "- No dropout because we don't want the output to be random\n",
    "\n",
    "- **/=keep_prob** makes **the test** without a scaling function to recover the output values into their expected range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Why does Dropout Prevent Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition:**\n",
    "\n",
    "- **Each hidden unite cannot rely on any one feature, so has to spread out weights**, which means weights can be widely spread out to all inputs. As a result, the final weight norm will be shrank.\n",
    "\n",
    "    - Shrinking the weight norm is silimar to the $L_{2}$ regularization.\n",
    "    \n",
    "    - Dropout can be considered as an adaptive way without a regularization.\n",
    "    \n",
    "**Downside:**\n",
    "\n",
    "- The cost function $J$ is not well-defined. It won't monotonically descrease by using dropout. \n",
    "    - Solutions: before using dropout, plot the cost function vs the number of iteration to check if the cost decreases as the iteration increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Other Regularization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Augmentation:**\n",
    "\n",
    "- Add more fake training samples by transforming. rotating, or distorting images.\n",
    "\n",
    "    ![](./imgs/data-aug.jpg)\n",
    "\n",
    "\n",
    "**Early Stopping:**\n",
    "\n",
    "- Stop halfway when the dev set error is the minimal\n",
    "\n",
    "    - The Forbenius Norm is around middle size, while its size will be large when overfitting, and closed to 0 when the iteration starts.\n",
    "    \n",
    "\n",
    "- Advantage: one only needs to try small, mid-, and large size of weight norms rather than trying a number of $\\lambda$ in $L_{2}$ regularization. \n",
    "    \n",
    "- Downside: it like solves the high variance and high bias at the same time, which will makes the problem more complicated. \n",
    "\n",
    "    ![](./imgs/early-stopping.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setting up your Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Normalizing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scale the test set using the same parameters $u,\\sigma^{2}$ from the train set so that these two sets are doing the same transformation!**\n",
    "\n",
    "$$ X = \\frac{X - u}{\\sigma} $$\n",
    "\n",
    "![](./imgs/normalize.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Normalize Inputs?**\n",
    "\n",
    "- If not, the cost function will be more irregular, becoming more complex to optimize. The learning rate should also be set very small. \n",
    "\n",
    "    ![](./imgs/why-norm.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Vanishing/Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**\n",
    "\n",
    "- In the very deep and deep neural networks, the gradients may become exponentially large or small, which is called vanishing / exploding gradients. \n",
    "\n",
    "- The reason is that if the weight matrix is a little larger or smaller than the identity matrix, the results may become exponentially large $2^{L}$ or small $\\frac{1}{2^{L}}$. \n",
    "\n",
    "    ![](./imgs/vanishing-exploding.jpg)\n",
    "    \n",
    "- Solution: **carefully initialize parameters**    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Weight Initialization for Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change Initial Variance:**\n",
    "\n",
    "- Change the initial variance of the weight matrix from 1 to $\\frac{1}{n^{[l-1]}}$ so that the linear combinition of $Z=WA+b$ won't become too large. \n",
    "\n",
    "```python\n",
    "# before \n",
    "W[l] = np.random.randn(shape) * 0.01\n",
    "\n",
    "# now \n",
    "W[l] = np.random.randn(shape) * np.sqrt(1/n[l]) \n",
    " \n",
    "```\n",
    "\n",
    "In practice: \n",
    "\n",
    "- $ReLU()$ function works well when the variance is set up as $\\frac{2}{n^{[l-1]}}$.\n",
    "\n",
    "- $tanh()$ function can use $\\sqrt{\\frac{1}{n^{[l-1]}}} $ or $\\sqrt{\\frac{2}{n^{[l-1]} + n^{[l]}}} $ \n",
    "\n",
    "- [See more details on this](https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization?source=post_page---------------------------) \n",
    "\n",
    "\n",
    "One can also treat this variance as a hyperparameter to tune in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Numerical Approximation of Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematical Idea:**\n",
    "\n",
    "$$ f^{'}(\\theta) = \\lim_{\\epsilon \\rightarrow 0} \\frac{f(\\theta + \\epsilon) - f(\\theta - \\epsilon)}{2\\epsilon } $$\n",
    "\n",
    "\n",
    "**In Practice:**\n",
    "\n",
    "- Two-Side Gradient Checking:\n",
    "\n",
    "$$ f^{'}(\\theta) \\approx \\frac{f(\\theta + \\epsilon) - f(\\theta - \\epsilon)}{2\\epsilon } $$\n",
    "\n",
    "- One-Side Gradient Checking:\n",
    "\n",
    "$$ f^{'}(\\theta) \\approx \\frac{f(\\theta + \\epsilon) - f(\\theta)}{\\epsilon} $$\n",
    "\n",
    "\n",
    "**Why use two-side rather than one-side grad check?**\n",
    "\n",
    "- The two-side check is $O(\\epsilon^2)$ while the one-side is $O(\\epsilon)$. Thus, the difference, i.e. 0.01, becomes larger in $O(\\epsilon^{2})$.  \n",
    "\n",
    "\n",
    "- Example:\n",
    "\n",
    "    ![](./imgs/gradient-check.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Grad Check:**\n",
    "\n",
    "- Reshape all weights $W^{[l]}, b^{[l]}$ into a gaint vector $\\theta$ and all gradients $dW^{[l]}, db^{[l]}$ into anothre gaint vector $d\\theta$. Now the cost function looks like: \n",
    "\n",
    "$$ J(\\theta) = J(\\theta_{1}, ..., \\theta_{L}) $$\n",
    "\n",
    "- For each $i$, compute:\n",
    "\n",
    "$$ d\\theta_{apporx}[i] = \\frac{J(\\theta_{1}, ..., \\theta{i} + \\epsilon, ...) - J(\\theta_{1}, ..., \\theta{i} - \\epsilon, ...)}{2\\epsilon} $$\n",
    "\n",
    "Ideally, as said above: \n",
    "\n",
    "$$ d\\theta_{approx}[i] \\approx d\\theta[i] $$\n",
    "\n",
    "- Use a threshold to check: \n",
    "\n",
    "    $$ \\frac{ ||d\\theta_{approx} - d\\theta||_{2}}{||d\\theta_{approx}||_{2} + ||d\\theta||_{2}} $$\n",
    "\n",
    "    - if the differece $\\approx 10^{-7}$, it's great!\n",
    "    - if the differece $\\approx 10^{-5}$, need to check!\n",
    "    - if  the differece $\\geq 10^{-3}$, it's wrong!\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "- Do not use Grad Check in training - only to debug.\n",
    "\n",
    "- If algorithm fails grad check, look at each individual component to try to identify bug. \n",
    "\n",
    "- Remeber regularizatin which addes the regularization term in the cost function.\n",
    "\n",
    "- Doesn't work with dropout which randomly select hidden units. \n",
    "\n",
    "- Run at random initialization; perhaps again after some training which will make parametrs wondering away from 0. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5%, and a dev set error of 7%. Which of the following are promising things to try to improve your classifier? (Check all that apply.)\n",
    "\n",
    "    **Increase the regularization parameter lambda.**\n",
    "\n",
    "**Why:**\n",
    "\n",
    "- The reason is that in the cost function, if the lambda is increased, the weight norm will be put more attention for minimizing the cost function. As a result, the weight norm will be reduced to around 0, also known as \"Weight Decat\". \n",
    "\n",
    "- Once the weight norm is around 0, many hidden units will die (have few impacts), so the deep neural network can be considered as a simple logistic regression, which has very high bias but less flexibility (underfitting). \n",
    "\n",
    "- So, **from overfitting to underfitting, one can just increase the regularization parameter lamdba**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. With the inverted dropout technique, at test time:\n",
    "\n",
    "    **You do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training.**\n",
    "    \n",
    "**Why?**    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TensorFlow and Deep Learning without a PhD: [Part_1](https://www.youtube.com/watch?v=u4alGiomYP4) and [Part_2](https://www.youtube.com/watch?v=fTUwdXUFfI8)\n",
    "\n",
    "- deeplearning.ai: [initialization](http://www.deeplearning.ai/ai-notes/initialization/)\n",
    "\n",
    "- CS231n: [Setting up the data and the model](http://cs231n.github.io/neural-networks-2/#reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zero Initialization: \n",
    "\n",
    "    - In general, initializing all the weights to zero results in the network **failing to break symmetry**. This means that **every neuron in each layer will learn the same thing**, and you might as well be training a neural network with $n^{[l]}=1$ for every layer, and the network is no more powerful than a linear classifier such as logistic regression. \n",
    "    \n",
    "    ![](./imgs/hw1-zero.png)\n",
    "    \n",
    "    \n",
    "- Large Random Initialization: \n",
    "    \n",
    "    - **Random initialization is used to break symmetry** and make sure different hidden units can learn different things\n",
    "    - Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. \n",
    "    - If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.\n",
    "    \n",
    "    ![](./imgs/hw1-large.png)\n",
    "    \n",
    "\n",
    "- He Initialization: \n",
    "\n",
    "    -He initialization works well for networks with ReLU activations.\n",
    "\n",
    "    ![](./imgs/hw1-he.png)\n",
    "    \n",
    "\n",
    "- Accuracy: \n",
    "\n",
    "    ![](./imgs/hw1-acc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember from this notebook**:\n",
    "- Different initializations lead to different results\n",
    "- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n",
    "- Don't intialize to values that are too large\n",
    "- He initialization works well for networks with ReLU activations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**\n",
    "\n",
    "- You have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France's goal keeper should kick the ball so that the French team's players can then hit it with their head.\n",
    "\n",
    "    ![](./imgs/hw2-prob.png)\n",
    "  \n",
    "  \n",
    "**Data:** \n",
    "\n",
    "![](./imgs/hw2-data.png)\n",
    "    \n",
    "- Each dot corresponds to a position on the football field where a football player has hit the ball with his/her head after the French goal keeper has shot the ball from the left side of the football field.\n",
    "    - If the dot is blue, it means the French player managed to hit the ball with his/her head\n",
    "    - If the dot is red, it means the other team's player hit the ball with their head\n",
    "\n",
    "**Goal**: \n",
    "\n",
    "- Use a deep learning model to find the positions on the field where the goalkeeper should kick the ball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-regularized Model:**\n",
    "\n",
    "- The non-regularized model is obviously overfitting the training set. It is fitting the noisy points! \n",
    "\n",
    "    ![](./imgs/hw2-non-reg.png)\n",
    "    \n",
    "\n",
    "**$L_{2}$ Regularization:**\n",
    "\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} $$\n",
    "\n",
    "- No overfitting any more! \n",
    "- The value of $\\lambda$ is a hyperparameter that you can tune using a dev set.\n",
    "- **L2 regularization makes your decision boundary smoother**. If $\\lambda$ is too large, it is also possible to \"oversmooth\", resulting in a model with high bias (underfitting).\n",
    "\n",
    "    ![](./imgs/hw2-l2.png)\n",
    "\n",
    "\n",
    "**Dropout:**\n",
    "\n",
    "- **It randomly shuts down some neurons in each iteration.**\n",
    "- Backward propogation: \n",
    "    - You had previously shut down some neurons during forward propagation, by applying a mask $D^{[1]}$ to `A1`. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask $D^{[1]}$ to `dA1`. \n",
    "    - During forward propagation, you had divided `A1` by `keep_prob`. In backpropagation, you'll therefore have to divide `dA1` by `keep_prob` again (the calculus interpretation is that if $A^{[1]}$ is scaled by `keep_prob`, then its derivative $dA^{[1]}$ is also scaled by the same `keep_prob`).\n",
    "\n",
    "- Dropout works great! The test accuracy has increased again (to 95%)! Your model is not overfitting the training set and does a great job on the test set.\n",
    "- A **common mistake** when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training. \n",
    "- Deep learning frameworks like [tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [PaddlePaddle](http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html), [keras](https://keras.io/layers/core/#dropout) or [caffe](http://caffe.berkeleyvision.org/tutorial/layers/dropout.html) come with a dropout layer implementation. Don't stress - you will soon learn some of these frameworks.\n",
    "\n",
    "    ![](./imgs/hw2-dropout.png)\n",
    "\n",
    "\n",
    "**Accuracy:**\n",
    "\n",
    "![](./imgs/hw2-acc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember about dropout:**\n",
    "- Dropout is a regularization technique.\n",
    "- You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.\n",
    "- Apply dropout both during forward and backward propagation.\n",
    "- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "$$ grad = \\frac{\\partial J}{\\partial \\theta} \\approx \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} = gradapprox\\tag{2}$$\n",
    "\n",
    "To check: \n",
    "\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{3}$$\n",
    "\n",
    "\n",
    "![](./imgs/hw3-gc.png)\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "- Gradient Checking is slow! Approximating the gradient with $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ is computationally costly. For this reason, we don't run gradient checking at every iteration during training. **Just a few times to check if the gradient is correct**. \n",
    "- Gradient Checking, at least as we've presented it, doesn't work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember from this notebook**:\n",
    "- Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).\n",
    "- Gradient checking is slow, so we don't run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
