{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Classic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LeNet:**\n",
    "\n",
    "![](./imgs/lenet.png)\n",
    "\n",
    "- Used sigmoid/tanh rather than ReLU\n",
    "- Back then, in convolutions, n_C in&out requires great computaional power, so LeNet used different filters to different channels for reducing that. \n",
    "- Had nonlinearity after pooling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlexNet:**\n",
    "\n",
    "![](./imgs/alexnet.png)\n",
    "- Much more parameters trained.\n",
    "- Used ReLU\n",
    "- Multiple GPUs\n",
    "- Local Response Normalization (LRN), normalizing all the channels of one position for reducing the number of neurons with high activations. *Doesn't help that much.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VGG-16:**\n",
    "\n",
    "![](./imgs/vgg16.png)\n",
    "- Simplified the architecture by using the same convolutions and paddings.\n",
    "    - CONV = 3 * 3 filter, s = 1, same\n",
    "    - MAX-POOL = 2 * 2, s = 2\n",
    "- Much more systematically designed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ResNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def:**\n",
    "\n",
    "![](./imgs/resnet-block.png)\n",
    "- To overcome the problems of vanishing and exploding gradient\n",
    "- Have very deep neural networks \n",
    "\n",
    "\n",
    "**Plain -> Residual Networks:**\n",
    "\n",
    "![](./imgs/plain2res.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why ResNets Work:**\n",
    "\n",
    "![](./imgs/whyResnet.png)\n",
    "- If use $L_{2}$ regularization, the weights are gonna shrink. It's very easy to get small weights and biases. If $W^{l+2} = 0$ and $b^{l+2} = 0$, $a^{l+2} = a^{l}$. \n",
    " - **it doesn't hurt the performance**\n",
    " - **skipping connections makes the networks easy to learn the identity functions** \n",
    " \n",
    "- Assuming the size of $a^{l+2}$ equals the size of $a^{l}$, it often use SAME convolutions. If in some cases, the diminsions are not same, weights can be added $a^{l}$ so as to perserve the same dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 1x1 Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def:**\n",
    "\n",
    "![](./imgs/conv1x1.png)\n",
    "\n",
    "- Applications: to **shrink the number of channels**\n",
    "![](./imgs/conv1x1-app.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Incecption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivations:**\n",
    "\n",
    "- Do not know which size to pick about convolutions and max-poolings. Thus, **let the network to help us choose**\n",
    "- Inception is basically to stack all the possible attempts together as long as the dimension are same (same convolutions)\n",
    "\n",
    "![](./imgs/moti-inception.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computional Cost:**\n",
    "\n",
    "- to reudce cost: use 1x1 convolutions to create an intermediate module (*bottleneck layer*)\n",
    "\n",
    "![](./imgs/inception-cost1.png)\n",
    "![](./imgs/inception-cost2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def:**\n",
    "\n",
    "- Concatenate all the channels together\n",
    "- Also add some softmax branches in the middle of the neural networks. \n",
    "- Memo: **WE NEED TO GO DEEPER**\n",
    "\n",
    "![](./imgs/inception-module.png)\n",
    "![](./imgs/inception-chain.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Practical Advices for using ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Using Open-Source Implementation\n",
    "\n",
    "- Get public codes from GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transfer Learning\n",
    "\n",
    "![](./imgs/transfer-learning.png)\n",
    "\n",
    "1. Freeze/Fix all the layers before the final softmax, and then only train the softmax layer for prediction.\n",
    "2. Freeze the first several layers (sharable low-level features), and then train the rest layers or design our own layers to train\n",
    "3. Treat all the weights as an initializtion, and then train the whole network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Augmentation\n",
    "\n",
    "**Common augmentation method:**\n",
    "- Mirroring\n",
    "- Random cropping\n",
    "- Rotation\n",
    "- Shearing\n",
    "- Local warping\n",
    "- **Color shifting**\n",
    "    - PCA Color Augmentation\n",
    "    \n",
    "![](./imgs/data-aug1.png)\n",
    "![](./imgs/data-aug2.png)\n",
    "\n",
    "**Implementing distortions:**\n",
    "- Multiple threads to load images and then compute distortions\n",
    "- Can parallelly do CPU/GPU training \n",
    "\n",
    "![](./imgs/data-aug3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- . -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No assignment in this week. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
