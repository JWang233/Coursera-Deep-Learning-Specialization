{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CV Problems:**\n",
    "- Image classification\n",
    "- Object detection (what & where)\n",
    "- Neural style transfer\n",
    "\n",
    "\n",
    "**Challenge:**\n",
    "- Large images (millions of weights) \n",
    "- CNN to fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution Operations:**\n",
    "- In deep learning, maybe not set the nine numbers in the filters by humans but learn the weights by nueral networks. \n",
    "- Thus, nueral networks can learn the low-level features in the image.\n",
    "\n",
    "![](./imgs/Convolution_schematic.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downsides of Convolutions:**\n",
    "- Image shrink every time you apply a convolution\n",
    "    - n * n image by f * f operation --> n-f+1 edge \n",
    "- Pixels on the edges are used less than the pixels on the center.\n",
    "\n",
    "**Padding:**\n",
    "- Add zeros around the images, so the image size can be preseved after convolutional operations.\n",
    "    - n + 2p - f + 1, where p is the padding number, indicating how many pixels are added to the boundary. \n",
    "    \n",
    "**Valid and Same Convolutions:**\n",
    "- Valid: as long as n > p \n",
    "- Same: the output image have the same size of the input image\n",
    "\n",
    "![](./imgs/PAD.png)\n",
    "*Padding of 2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Strided Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def:**\n",
    "- Skip a step ***S*** of pixels when using convolutions\n",
    "- $$ \\frac{n + 2p - f}{S} + 1 $$\n",
    "\n",
    "![](./imgs/summary_con.png)\n",
    "\n",
    "***This formular is valid for padding as well!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Convolutions Over Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def on RGB Images:**\n",
    "- Change the filter to 3D\n",
    "- Output is still 1D because of the sum of all combinitions\n",
    "\n",
    "**Multiple Filters:**\n",
    "- Do the RGB 3D way for each filters\n",
    "- Add all the output together at the end   $n'*n'*n_{num_filters}$\n",
    "\n",
    "![](./imgs/multifilter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 One Layer of a Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notations:**\n",
    "\n",
    "If layer $l$ is a current convolution layer, \n",
    "- $f^{[l]}$ = filter size \n",
    "- $p^{[l]}$ = padding \n",
    "- $s^{[l]}$ = stride size \n",
    "- $n_{C}^{[l]}$ = number of filters \n",
    "\n",
    "\n",
    "Given input image size\n",
    "\n",
    "$$ n_{H}^{[l-1]} \\times n_{W}^{[l-1]} \\times n_{C}^{[l-1]} $$\n",
    "\n",
    "Output size is \n",
    "\n",
    "$$ n_{H}^{[l]} \\times n_{W}^{[l]} \\times n_{C}^{[l]} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ n_{H}^{[l]} = floor ( \\frac{n_{H}^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1) $$\n",
    "\n",
    "\n",
    "In layer $l$, \n",
    "- Each fitler is: $f^{[l]} \\times f^{[l]} \\times n_{C}^{[l-1]}$\n",
    "- Activations: $a^{[l]} -> n_{H}^{[l]} \\times n_{W}^{[l]} \\times n_{C}^{[l]} $\n",
    "- Weights: $ f^{[l]} * f^{[l]} * n_{C}^{[l-1]} * n_{C}^{[l]} $\n",
    "- Bias: $ n_{C}^{[l]} $\n",
    "\n",
    "\n",
    "![](./imgs/eg-conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LeNet5**\n",
    "\n",
    "![](./imgs/lenet5.png)\n",
    "\n",
    "![](./imgs/eg-convparm.png)\n",
    "*Wrong numbers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def:**\n",
    "- Only two fixed parameters: size (square) & stride\n",
    "    - No parameters to learn!\n",
    "- Two types \n",
    "    - max : used much more\n",
    "    - average : in some very deep layers to collect representations effectively    \n",
    "\n",
    "\n",
    "**Intuitive:**\n",
    "- No rigid proofs but intuitively, say max pooling, it marks feature locations\n",
    "- If features detected anywhere in this filter, then keep a high number; if not, the max pooling is also quite small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Why Convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Sharing:**\n",
    "- A feature detector (e.g. edge) that is useful in the one part of the image is prbabbly useful in another part of the image\n",
    "\n",
    "**Sparsity of Connections:**\n",
    "- In each layer, each output value depends only on a small number of inputs.\n",
    "- In other words, the target pixel only depends on its filtered neighboring pixels\n",
    "\n",
    "**Translation Invariance:**\n",
    "- because the same filter is applied to everywhere of an image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Because pooling layers do not have parameters, they do not affect the backpropagation (derivatives) calculation.**\n",
    "\n",
    "***FALSE***\n",
    "- The pooling layers do not have any parameters, so there is nothing for back propagation to change about them. So \"back propagation does not affect the pooling layers\"\n",
    "- Back propagation must pass backwards through the pooling layers and the way it works depends on the type of pooling. For max pooling, the gradients are applied only to the maximum of the inputs to the pooling layer. For average pooling, the gradients are applied proportionally to all the inputs. So pooling layers do affect back propagation: something happens at the pooling layers during back propagation.\n",
    "\n",
    "*By Paul Mielke*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Which of the following statements about parameter sharing in ConvNets are true? (Check all that apply.)**\n",
    "- It allows gradient descent to set many of the parameters to zero, thus making the connections sparse. ***FALSE***\n",
    "- It reduces the total number of parameters, thus reducing overfitting. ***TRUE***\n",
    "- It allows parameters learned for one task to be shared even for a different task (transfer learning). ***FALSE***\n",
    "- It allows a feature detector to be used in multiple locations throughout the whole input image/input volume. ***TRUE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implemented forward- and back-prop CNN!**\n",
    "- How backprop is implemented in convolution, maxpooling, and averagepooling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
