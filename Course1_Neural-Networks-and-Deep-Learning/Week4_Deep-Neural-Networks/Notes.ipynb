{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Notations in Deep L-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notations:**\n",
    "\n",
    "<img src = 'imgs/dnn-notations.jpg'>\n",
    "\n",
    "- Number of layers: $L$\n",
    "\n",
    "- Number of units in layer $l$ : $n^{[l]} $ \n",
    "\n",
    "$$ n^{[0]} = n_{x} = 3, \\; n^{[1]} = n^{[2]} = 5, \\; n^{[3]} = 3, \\; n^{[4]} = n^{[L]} = 1 $$\n",
    "\n",
    "- Outputs of activation functions in layer $l$ : $ a^{[l]} $\n",
    "\n",
    "$$ a^{[l]} = g^{[l]} \\; (Z^{[l]}) $$ \n",
    "\n",
    "$$ \\text{where} \\; a^{[0]} = x, \\; \\hat{y} = a^{[L]} $$\n",
    "\n",
    "- Parametres and outputs in layer $l$ :\n",
    "\n",
    "$$ W^{[l]}, \\; b^{[l]} \\; for Z^{[l]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Vectorized Forward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization:**\n",
    "\n",
    "- for $i = 1, ..., L:$ \n",
    "\n",
    "$$ Z^{[i]} = W^{[i]} A^{[i-1]} + b^{[i]} $$\n",
    "\n",
    "$$ A^{[i]} = g^{[i]} \\; (Z^{[i]}) $$\n",
    "\n",
    "- The for-loop for each layer is unavoidable in the vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debugging Using the Matrix Dimensions:**\n",
    "\n",
    "- Parameters $ W^{[l]}$ and $b^{[l]}$: \n",
    "\n",
    "$$ Z^{[l]} : (n^{[l]}, \\;m) $$\n",
    "\n",
    "$$ W^{[l]} : (n^{[l]}, \\; n^{[l-1]}) $$ \n",
    "\n",
    "$$ b^{[l]} : (n^{[l]}, \\; 1) \\text{, but will be broadcasted to } \\; (n^{[l]}, m)$$\n",
    "$$ \\text{it means each node/row only needs one bias} $$\n",
    "\n",
    "- The output in each layer is kind equal to the number of nodes in that layer. It means each node is going to output one row of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Why Deep Network Works Well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Face Recognization:**\n",
    "\n",
    "<img src = 'imgs/whydnn-face.jpg'>\n",
    "\n",
    "- The First layer may produce low-level features, such as edges. Then, the following layers are going to group these low-level features together, i.e. generating some parts of face such as eyes, nose, etc. Finally, a face can be built up using different parts of face for detecting faces.\n",
    "\n",
    "\n",
    "**Circuit Theory:**\n",
    "\n",
    "<img src = 'imgs/whydnn-circuit.jpg'>\n",
    "\n",
    "- There are functions you can compute with a \"small\" L-layer deep neural network $O(\\log n)$ that shallower networks $O(2^{n})$ require exponentially more hidden units to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Blocks of DNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In layer $l$ : \n",
    "\n",
    "- Initialize $W^{[l]}$, \\; $b^{[l]}$\n",
    "\n",
    "- Forward: input $a^{[l-1]}$, output $a^{[l]}$, cache $Z^{[l]}, \\; W^{[l]}, \\;  b^{[l]} $ \n",
    "    \n",
    "    where  $Z^{[l]}%$, $W^{[l]}$ and $b^{[l]}$ are cached during forward propagation to the corresponding backward propagation step, which contain useful values for backward propagation to compute derivatives. \n",
    "\n",
    "- Backward: input $ da^{[l]}$ and the cached data, output $ da^{[l-1]}, \\; dW^{[l]},  \\; db^{[l]} $ \n",
    "\n",
    "- Update: $ W^{[l]} = W^{[l]} - \\alpha \\; dW^{[l]} $ and $ b^{[l]} = b^{[l]} - \\alpha \\; db^{[l]} $ \n",
    "\n",
    "<img src = 'imgs/dnn-blocks.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Forward and Backward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propogation for layer $l$ :\n",
    "\n",
    "$$ Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]} $$\n",
    "\n",
    "$$ A^{[l]} = g^{[l]} \\; (Z^{[l]}) $$\n",
    "\n",
    "- Backward propogation for layer $l$ :\n",
    "\n",
    "$$ dZ^{[l]} = dA^{[l]} \\ast g^{[l]\\;'}\\;(Z^{[l]}) $$\n",
    "\n",
    "$$ dW^{[l]} = \\frac{1}{m} \\; dZ^{[l]} A^{[l-1]\\;T} $$\n",
    "\n",
    "$$ db^{[l]} = \\frac{1}{m} \\; \\text{np.sum( $dZ^{[l]}$, axis=1, keepdim=True)} $$\n",
    "\n",
    "$$ dA^{[l-1]} = W^{[l]\\;T} dZ^{[l]} $$ \n",
    "\n",
    "    where (if we use the sigmoid function in the output layer) \n",
    "$$dA^{[L]} = \\frac{1}{m} \\; \\sum_{i=1}^{m}(-\\frac{y^{(i)}}{a^{(i)}} + \\frac{1-y^{(i)}}{1-a^{(i)}}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Parameters vs Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "\n",
    "- $W^{[l]}$ and $b^{[l]}$\n",
    "\n",
    "**Hyperparameters:**\n",
    "\n",
    "- Learning rate: $\\alpha$\n",
    "\n",
    "- Number of iterations of Gradient Descent\n",
    "\n",
    "- Number of layers: $L$\n",
    "\n",
    "- Number of hidden unites: $n^{[l]}$\n",
    "\n",
    "- Choice of activation function\n",
    "\n",
    "- Momentum\n",
    "\n",
    "- Minibatch\n",
    "\n",
    "- Size\n",
    "\n",
    "- regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the **optimal** hyperparameters is a very **empirical process**. One has to try out a range of settings for differernt datasets.\n",
    "\n",
    "<img src = 'imgs/empirical.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actication function is one of the hyperparameters but the activation values are not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building your Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model:**\n",
    "\n",
    "<img src = 'imgs/hw-model.png'>\n",
    "\n",
    "<img src = 'imgs/hw-model1.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Design:**\n",
    "\n",
    "- Initialize: \n",
    "\n",
    "    - Use a varaible *layer_dims* to record the size of each layer. Specifically, *layer_dims[0]* stores the size of each input feature, also known as $n^{[0]}$. \n",
    "    \n",
    "    - Then, use a helper functions to store all initial parameters in to a varibale. Using a dictionary is better for retrieve and store, i.e. \n",
    "    \n",
    "    ``` Python\n",
    "        parameters['W' + str(l)] = ...\n",
    "        parameters['b' + str(l)] = ... # column is always one because of python broadcasting \n",
    "    ```    \n",
    "\n",
    "- Forward: \n",
    "\n",
    "    - First do linear combination in a helper function. Note that the output $Z$ and parameters $W,b$ should be cached in a variable *linear_cache*.\n",
    "    \n",
    "    $$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "    \n",
    "    - Then, use another helper function to compute the activation value, the output $A$ and input $Z$ should also be cached in a variable *activation_cache*.  \n",
    "    \n",
    "    - Finally, use a for loop to go over each hidden layer, ended up with the last output layer.\n",
    "\n",
    "    <img src = 'imgs/hw-lnn.png'>\n",
    "\n",
    "\n",
    "- Backward: \n",
    "    \n",
    "    - create a helper function to compute the gradients in each layer, including $dW^{[l]}, db^{[l]}, \\text{and} \\; dA^{[l-1]}$. The input parameters only require $dZ \\; \\text{and} \\; linear_cache$\n",
    "    \n",
    "    - use the same programming idea to compute $dZ$ for each layer: \n",
    "        - first compute the $dZ$ from the output layer\n",
    "        - then loop all the other hidder layers \n",
    "        - store the gradients into a vairable *grads*\n",
    "\n",
    "    - finally, update all the parameters in another loop\n",
    "    \n",
    "    <img src = 'imgs/hw-lnn-back.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Neural Network - Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**\n",
    "\n",
    "- Cat Classification \n",
    "\n",
    "    <img src = 'imgs/hw2-problem.png'>\n",
    "\n",
    "**Model:**\n",
    "\n",
    "- 2 layer neural network \n",
    "\n",
    "- L-layer deep neural network\n",
    "\n",
    "    <img src = 'imgs/hw2-model.png'>\n",
    "\n",
    "**Results:** \n",
    "\n",
    "- 2 layer NN: \n",
    "\n",
    "    ```python\n",
    "    Cost after iteration 0: 0.6930497356599888\n",
    "    Cost after iteration 100: 0.6464320953428849\n",
    "    Cost after iteration 200: 0.6325140647912677\n",
    "    Cost after iteration 300: 0.6015024920354665\n",
    "    Cost after iteration 400: 0.5601966311605747\n",
    "    Cost after iteration 500: 0.515830477276473\n",
    "    Cost after iteration 600: 0.4754901313943325\n",
    "    Cost after iteration 700: 0.4339163151225749\n",
    "    Cost after iteration 800: 0.4007977536203887\n",
    "    Cost after iteration 900: 0.3580705011323798\n",
    "    Cost after iteration 1000: 0.3394281538366412\n",
    "    Cost after iteration 1100: 0.3052753636196264\n",
    "    Cost after iteration 1200: 0.27491377282130164\n",
    "    Cost after iteration 1300: 0.24681768210614846\n",
    "    Cost after iteration 1400: 0.19850735037466116\n",
    "    Cost after iteration 1500: 0.1744831811255664\n",
    "    Cost after iteration 1600: 0.17080762978096148\n",
    "    Cost after iteration 1700: 0.11306524562164734\n",
    "    Cost after iteration 1800: 0.09629426845937152\n",
    "    Cost after iteration 1900: 0.08342617959726863\n",
    "    Cost after iteration 2000: 0.07439078704319081\n",
    "    Cost after iteration 2100: 0.0663074813226793\n",
    "    Cost after iteration 2200: 0.0591932950103817\n",
    "    Cost after iteration 2300: 0.053361403485605585\n",
    "    Cost after iteration 2400: 0.04855478562877016\n",
    "        \n",
    "    Training Accuracy: 1.0\n",
    "    Testing Accuracy: 0.72\n",
    "    ```\n",
    "    \n",
    "    <img src = 'imgs/hw2-res-2nn.png'>\n",
    "    \n",
    "    \n",
    "- L-layer dnn: \n",
    "    \n",
    "    ```python \n",
    "    Cost after iteration 0: 0.695046\n",
    "    Cost after iteration 100: 0.589260\n",
    "    Cost after iteration 200: 0.523261\n",
    "    Cost after iteration 300: 0.449769\n",
    "    Cost after iteration 400: 0.420900\n",
    "    Cost after iteration 500: 0.372464\n",
    "    Cost after iteration 600: 0.347421\n",
    "    Cost after iteration 700: 0.317192\n",
    "    Cost after iteration 800: 0.266438\n",
    "    Cost after iteration 900: 0.219914\n",
    "    Cost after iteration 1000: 0.143579\n",
    "    Cost after iteration 1100: 0.453092\n",
    "    Cost after iteration 1200: 0.094994\n",
    "    Cost after iteration 1300: 0.080141\n",
    "    Cost after iteration 1400: 0.069402\n",
    "    Cost after iteration 1500: 0.060217\n",
    "    Cost after iteration 1600: 0.053274\n",
    "    Cost after iteration 1700: 0.047629\n",
    "    Cost after iteration 1800: 0.042976\n",
    "    Cost after iteration 1900: 0.039036\n",
    "    Cost after iteration 2000: 0.035683\n",
    "    Cost after iteration 2100: 0.032915\n",
    "    Cost after iteration 2200: 0.030472\n",
    "    Cost after iteration 2300: 0.028388\n",
    "    Cost after iteration 2400: 0.026615\n",
    "    \n",
    "    Training Accuracy: 1.0\n",
    "    Testing Accuracy: 0.8\n",
    "    ```\n",
    "    \n",
    "    <img src = 'imgs/hw2-res-dnn.png'>\n",
    "\n",
    "\n",
    "**Results Analysis:**\n",
    "\n",
    "**A few types of images the model tends to do poorly on include** \n",
    "- Cat body in an unusual position\n",
    "\n",
    "- Cat appears against a background of a similar color\n",
    "\n",
    "- Unusual cat color and species\n",
    "\n",
    "- Camera Angle\n",
    "\n",
    "- Brightness of the picture\n",
    "\n",
    "- Scale variation (cat is very large or small in image) \n",
    "\n",
    "<img src = 'imgs/hw2-resana.png'>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
